{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Количество используемых слоёв VGG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T09:59:57.603945Z",
     "iopub.status.busy": "2025-03-15T09:59:57.603743Z",
     "iopub.status.idle": "2025-03-15T09:59:57.607924Z",
     "shell.execute_reply": "2025-03-15T09:59:57.607042Z",
     "shell.execute_reply.started": "2025-03-15T09:59:57.603926Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T09:59:57.619434Z",
     "iopub.status.busy": "2025-03-15T09:59:57.619145Z",
     "iopub.status.idle": "2025-03-15T10:00:00.199504Z",
     "shell.execute_reply": "2025-03-15T10:00:00.198580Z",
     "shell.execute_reply.started": "2025-03-15T09:59:57.619406Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, ToPILImage\n",
    "from torchvision.models import vgg19\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T10:00:00.201208Z",
     "iopub.status.busy": "2025-03-15T10:00:00.200730Z",
     "iopub.status.idle": "2025-03-15T10:00:00.223694Z",
     "shell.execute_reply": "2025-03-15T10:00:00.222771Z",
     "shell.execute_reply.started": "2025-03-15T10:00:00.201177Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "torch.set_default_device(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Класс модели-стилизатора "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T10:00:00.225693Z",
     "iopub.status.busy": "2025-03-15T10:00:00.225414Z",
     "iopub.status.idle": "2025-03-15T10:00:00.361074Z",
     "shell.execute_reply": "2025-03-15T10:00:00.360120Z",
     "shell.execute_reply.started": "2025-03-15T10:00:00.225672Z"
    }
   },
   "outputs": [],
   "source": [
    "VGG19_NORMALIZATION_MEAN = torch.tensor([0.485, 0.456, 0.406])\n",
    "VGG19_NORMALIZATION_STD = torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "CONTENT_LAYERS_DEFAULT = ['Conv_4']\n",
    "STYLE_LAYERS_DEFAULT = ['Conv_1', 'Conv_2', 'Conv_3', 'Conv_4', 'Conv_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T10:00:00.362816Z",
     "iopub.status.busy": "2025-03-15T10:00:00.362490Z",
     "iopub.status.idle": "2025-03-15T10:00:00.369213Z",
     "shell.execute_reply": "2025-03-15T10:00:00.368463Z",
     "shell.execute_reply.started": "2025-03-15T10:00:00.362786Z"
    }
   },
   "outputs": [],
   "source": [
    "class ContentLoss(nn.Module):\n",
    "    def __init__(self, target):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        self.target = target.detach().to(device)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        self.loss = F.mse_loss(inp, self.target).to(device)\n",
    "        return inp\n",
    "    \n",
    "\n",
    "class StyleLoss(nn.Module):\n",
    "    def __init__(self, target):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = StyleLoss.gram_matrix(target).detach().to(device)\n",
    "        \n",
    "    @staticmethod\n",
    "    def gram_matrix(inp):\n",
    "        batch_size, nmaps, w, h = inp.size()\n",
    "    \n",
    "        features = inp.view(batch_size * nmaps, w * h) # Получаем для каждой карты признаков вектор размера wxh\n",
    "    \n",
    "        G = torch.mm(features, features.T)\n",
    "    \n",
    "        return G.div(batch_size * nmaps * w * h)\n",
    "\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        G = StyleLoss.gram_matrix(inp).to(device)\n",
    "        self.loss = F.mse_loss(G, self.target).to(device)\n",
    "        return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T10:00:00.370322Z",
     "iopub.status.busy": "2025-03-15T10:00:00.370091Z",
     "iopub.status.idle": "2025-03-15T10:00:00.384133Z",
     "shell.execute_reply": "2025-03-15T10:00:00.383287Z",
     "shell.execute_reply.started": "2025-03-15T10:00:00.370302Z"
    }
   },
   "outputs": [],
   "source": [
    "class Normalization(nn.Module):\n",
    "    def __init__(self, mean=VGG19_NORMALIZATION_MEAN,\n",
    "                 std=VGG19_NORMALIZATION_STD):\n",
    "       super(Normalization, self).__init__()\n",
    "       self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
    "       self.std = torch.tensor(std).view(-1, 1, 1)\n",
    "   \n",
    "    def forward(self, img):\n",
    "       return (img - self.mean) / self.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T10:00:00.385105Z",
     "iopub.status.busy": "2025-03-15T10:00:00.384857Z",
     "iopub.status.idle": "2025-03-15T10:00:00.397014Z",
     "shell.execute_reply": "2025-03-15T10:00:00.396146Z",
     "shell.execute_reply.started": "2025-03-15T10:00:00.385077Z"
    }
   },
   "outputs": [],
   "source": [
    "class StyleModel:\n",
    "    \"\"\"\n",
    "    Класс для создания интерфейса по переносу стиля\n",
    "    \"\"\"\n",
    "    def __init__(self, base_cnn, content_img, style_img, \n",
    "                 normalization_mean=VGG19_NORMALIZATION_MEAN, \n",
    "                 normalization_std=VGG19_NORMALIZATION_STD, \n",
    "                 content_layers=CONTENT_LAYERS_DEFAULT, \n",
    "                 style_layers=STYLE_LAYERS_DEFAULT):\n",
    "        \n",
    "        content_img = content_img.to(device)\n",
    "        style_img = style_img.to(device)\n",
    "        self.model = nn.Sequential().to(device)\n",
    "        self.content_losses = []\n",
    "        self.style_losses = []\n",
    "        \n",
    "        self.model.add_module(\"ImageNorm\", Normalization(mean=normalization_mean, std=normalization_std).to(device))\n",
    "\n",
    "        conv_counter: int = 0\n",
    "        module_name: str = \"\"\n",
    "        \n",
    "        for layer in base_cnn.children():\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                conv_counter += 1\n",
    "                module_name = f\"Conv_{conv_counter}\"\n",
    "                \n",
    "            elif isinstance(layer, nn.ReLU):\n",
    "                module_name = f\"ReLU_{conv_counter}\"\n",
    "                layer = nn.ReLU(inplace=False)\n",
    "                \n",
    "            elif isinstance(layer, nn.MaxPool2d):\n",
    "                module_name = f\"MaxPool2d_{conv_counter}\"\n",
    "            \n",
    "            else:\n",
    "                raise ValueError\n",
    "            \n",
    "            self.model.add_module(module_name, layer.to(device))\n",
    "\n",
    "            if module_name in content_layers:\n",
    "                target = self.model(content_img).detach().to(device)\n",
    "                content_loss_module = ContentLoss(target)\n",
    "                self.content_losses.append(content_loss_module)\n",
    "                self.model.add_module(f\"ContentLoss_{conv_counter}\", content_loss_module)\n",
    "                \n",
    "            if module_name in style_layers:\n",
    "                target = self.model(style_img).detach().to(device)\n",
    "                style_loss_module = StyleLoss(target)\n",
    "                self.style_losses.append(style_loss_module)\n",
    "                self.model.add_module(f\"StyleLoss_{conv_counter}\", style_loss_module)\n",
    "                \n",
    "        self.model.to(device)\n",
    "\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__.__name__}(model = {self.model})'\n",
    "\n",
    "\n",
    "    def transfer_style(self, input_image, optimizer_class=None, lr=0.05, \n",
    "                       num_steps=300, style_weight=100000, content_weight=1, print_logs:int|None=None):\n",
    "        \n",
    "        input_image = input_image.to(device)\n",
    "        \n",
    "        input_image.requires_grad = True\n",
    "        self.model.eval()\n",
    "\n",
    "        optimizer = None\n",
    "        if not optimizer_class:\n",
    "            optimizer = optim.Adam([input_image], lr=lr)\n",
    "        else:\n",
    "            optimizer = optimizer_class([input_image], lr=lr)\n",
    "\n",
    "        \n",
    "        for i in range(num_steps):\n",
    "            \n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                self.model(input_image)\n",
    "                content_final_loss = 0.0\n",
    "                style_final_loss = 0.0\n",
    "            \n",
    "                for content_loss in self.content_losses:\n",
    "                    content_final_loss += content_loss.loss\n",
    "\n",
    "                for style_loss in self.style_losses:\n",
    "                    style_final_loss += style_loss.loss\n",
    "\n",
    "                loss = content_weight * content_final_loss + style_weight * style_final_loss\n",
    "                loss.backward()\n",
    "\n",
    "                return loss\n",
    "                \n",
    "            optimizer.step(closure)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                input_image.clamp_(0, 1)\n",
    "\n",
    "            if print_logs:\n",
    "                if i % print_logs == 0:\n",
    "                    print(f\"Эпоха номер {i + 1}\")\n",
    "                    plt.imshow(ToPILImage()(input_image[0]))\n",
    "                    plt.show()\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            input_image.clamp_(0, 1)\n",
    "            \n",
    "        return input_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Входные данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T10:00:00.398150Z",
     "iopub.status.busy": "2025-03-15T10:00:00.397857Z",
     "iopub.status.idle": "2025-03-15T10:00:01.277528Z",
     "shell.execute_reply": "2025-03-15T10:00:01.276513Z",
     "shell.execute_reply.started": "2025-03-15T10:00:00.398120Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "IMSIZE = (512, 512)\n",
    "\n",
    "contents_directory = \"/kaggle/input/images3-0/data/contents/\"\n",
    "styles_directory = \"/kaggle/input/images3-0/data/styles/\"\n",
    "\n",
    "content_names = os.listdir(contents_directory) \n",
    "style_names = os.listdir(styles_directory)\n",
    "\n",
    "content_images = [Image.open(contents_directory + filename).resize(IMSIZE) for filename in content_names]\n",
    "style_images = [Image.open(styles_directory + filename).resize(IMSIZE) for filename in style_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Эксперименты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Функция эксперимента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T10:00:01.280327Z",
     "iopub.status.busy": "2025-03-15T10:00:01.280088Z",
     "iopub.status.idle": "2025-03-15T10:00:01.283808Z",
     "shell.execute_reply": "2025-03-15T10:00:01.282962Z",
     "shell.execute_reply.started": "2025-03-15T10:00:01.280306Z"
    }
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T10:00:01.285469Z",
     "iopub.status.busy": "2025-03-15T10:00:01.285248Z",
     "iopub.status.idle": "2025-03-15T10:00:01.296439Z",
     "shell.execute_reply": "2025-03-15T10:00:01.295762Z",
     "shell.execute_reply.started": "2025-03-15T10:00:01.285449Z"
    }
   },
   "outputs": [],
   "source": [
    "def experiment(base_model, content_images, style_images, \n",
    "               optimizer, lr, num_steps, \n",
    "               content_weight, style_weight, \n",
    "               content_layers, style_layers, \n",
    "               noisy_input: bool = False, print_logs: int|None=None) -> dict[str, float]:\n",
    "    \n",
    "    results: dict[str, float] = {\n",
    "        \"avg_time\": 0.0, \n",
    "        \"content_score\": 0.0,\n",
    "        \"style_score\": 0.0\n",
    "    }\n",
    "    \n",
    "    iteration_counter = 0\n",
    "    total_time_spent_on_transfering = 0.0\n",
    "\n",
    "    total_content_score = 0.0\n",
    "    total_style_score = 0.0\n",
    "    \n",
    "    for content_img in content_images:\n",
    "        for style_img in style_images:\n",
    "            \n",
    "            start = time.time()\n",
    "            \n",
    "            content = ToTensor()(content_img)[:3].unsqueeze(0)\n",
    "            style = ToTensor()(style_img)[:3].unsqueeze(0)\n",
    "        \n",
    "            input_ = torch.randn(content.data.size()) if noisy_input else content.clone()\n",
    "            \n",
    "            style_model = StyleModel(\n",
    "                base_cnn=base_model,\n",
    "                content_img=content,\n",
    "                style_img=style,\n",
    "                content_layers=content_layers,\n",
    "                style_layers=style_layers\n",
    "            )\n",
    "\n",
    "            output = style_model.transfer_style(\n",
    "                input_image=input_,\n",
    "                optimizer_class=optimizer,\n",
    "                lr=lr,\n",
    "                num_steps=num_steps,\n",
    "                content_weight=content_weight,\n",
    "                style_weight=style_weight,\n",
    "                print_logs=print_logs\n",
    "            )\n",
    "            \n",
    "            end = time.time()\n",
    "            total_time_spent_on_transfering += end - start\n",
    "            iteration_counter += 1\n",
    "\n",
    "            with torch.no_grad():\n",
    "                total_content_score += F.mse_loss(output.to(device), content.to(device)).item()\n",
    "                style_loss = StyleLoss(style)\n",
    "                style_loss(output)\n",
    "                total_style_score += style_loss.loss.item()\n",
    "\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(13, 16))\n",
    "            axes[0].set_title(\"Content\")\n",
    "            axes[1].set_title(\"Style\")\n",
    "            axes[2].set_title(\"Result\")\n",
    "            axes[0].imshow(content_img)\n",
    "            axes[1].imshow(style_img)\n",
    "            axes[2].imshow(ToPILImage()(output[0]))\n",
    "            plt.show()\n",
    "            \n",
    "    results[\"avg_time\"] = total_time_spent_on_transfering / iteration_counter\n",
    "    results[\"content_score\"] = total_content_score / iteration_counter\n",
    "    results[\"style_score\"] = total_style_score / iteration_counter\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Выбор количества слоёв VGG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Бейзлайновое решение - 11 слоев, 5 свёрток"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T10:00:01.297283Z",
     "iopub.status.busy": "2025-03-15T10:00:01.297050Z",
     "iopub.status.idle": "2025-03-15T10:00:01.866592Z",
     "shell.execute_reply": "2025-03-15T10:00:01.865699Z",
     "shell.execute_reply.started": "2025-03-15T10:00:01.297264Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_cnn = vgg19(pretrained=True).features[0 : 11].to(device)\n",
    "\n",
    "for param in base_cnn.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "base_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T10:00:01.867787Z",
     "iopub.status.busy": "2025-03-15T10:00:01.867454Z",
     "iopub.status.idle": "2025-03-15T10:00:01.871933Z",
     "shell.execute_reply": "2025-03-15T10:00:01.871087Z",
     "shell.execute_reply.started": "2025-03-15T10:00:01.867749Z"
    }
   },
   "outputs": [],
   "source": [
    "OPTIMIZER = optim.Adam\n",
    "LR = 0.05\n",
    "NUM_STEPS=400\n",
    "CONTENT_WEIGHT = 1\n",
    "STYLE_WEIGHT = 10**8\n",
    "CONTENT_LAYERS = CONTENT_LAYERS_DEFAULT\n",
    "STYLE_LAYERS = STYLE_LAYERS_DEFAULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = experiment(\n",
    "    base_model=base_cnn,\n",
    "    content_images=content_images[8:10], \n",
    "    style_images=style_images[4:10],\n",
    "    optimizer=OPTIMIZER, \n",
    "    lr=LR, \n",
    "    num_steps=NUM_STEPS, \n",
    "    content_weight=CONTENT_WEIGHT, \n",
    "    style_weight=STYLE_WEIGHT, \n",
    "    content_layers=CONTENT_LAYERS, \n",
    "    style_layers=STYLE_LAYERS,\n",
    "    noisy_input=False,\n",
    "    print_logs=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T10:41:40.428186Z",
     "iopub.status.busy": "2025-03-15T10:41:40.427840Z",
     "iopub.status.idle": "2025-03-15T10:41:40.433466Z",
     "shell.execute_reply": "2025-03-15T10:41:40.432699Z",
     "shell.execute_reply.started": "2025-03-15T10:41:40.428156Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avg_time': 10.7481,\n",
       " 'content_score': 0.0314144480210543,\n",
       " 'style_score': 0.0015353436165562}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. 17 слоёв (8 сверток)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T10:02:18.498986Z",
     "iopub.status.busy": "2025-03-15T10:02:18.498771Z",
     "iopub.status.idle": "2025-03-15T10:02:19.065306Z",
     "shell.execute_reply": "2025-03-15T10:02:19.064518Z",
     "shell.execute_reply.started": "2025-03-15T10:02:18.498967Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace=True)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_cnn = vgg19(pretrained=True).features[0 : 17].to(device)\n",
    "\n",
    "for param in base_cnn.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "base_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T10:02:19.066208Z",
     "iopub.status.busy": "2025-03-15T10:02:19.065969Z",
     "iopub.status.idle": "2025-03-15T10:02:19.070823Z",
     "shell.execute_reply": "2025-03-15T10:02:19.069773Z",
     "shell.execute_reply.started": "2025-03-15T10:02:19.066189Z"
    }
   },
   "outputs": [],
   "source": [
    "OPTIMIZER = optim.Adam\n",
    "LR = 0.05\n",
    "NUM_STEPS=400\n",
    "CONTENT_WEIGHT = 1\n",
    "STYLE_WEIGHT = 10**8\n",
    "CONTENT_LAYERS = ['Conv_7', 'Conv_8']\n",
    "STYLE_LAYERS = [f'Conv_{i}' for i in range(1, 9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = experiment(\n",
    "    base_model=base_cnn,\n",
    "    content_images=content_images[8:10], \n",
    "    style_images=style_images[4:10],\n",
    "    optimizer=OPTIMIZER, \n",
    "    lr=LR, \n",
    "    num_steps=NUM_STEPS, \n",
    "    content_weight=CONTENT_WEIGHT, \n",
    "    style_weight=STYLE_WEIGHT, \n",
    "    content_layers=CONTENT_LAYERS, \n",
    "    style_layers=STYLE_LAYERS,\n",
    "    noisy_input=False,\n",
    "    print_logs=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T10:43:10.586300Z",
     "iopub.status.busy": "2025-03-15T10:43:10.585954Z",
     "iopub.status.idle": "2025-03-15T10:43:10.591749Z",
     "shell.execute_reply": "2025-03-15T10:43:10.590855Z",
     "shell.execute_reply.started": "2025-03-15T10:43:10.586274Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avg_time': 15.597521662712097,\n",
       " 'content_score': 0.041549883272250496,\n",
       " 'style_score': 0.0005501802809611738}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Все свёрточные слои VGG (16 свёрток)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T10:05:33.397386Z",
     "iopub.status.busy": "2025-03-15T10:05:33.397142Z",
     "iopub.status.idle": "2025-03-15T10:05:33.945962Z",
     "shell.execute_reply": "2025-03-15T10:05:33.945179Z",
     "shell.execute_reply.started": "2025-03-15T10:05:33.397356Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace=True)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (17): ReLU(inplace=True)\n",
       "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU(inplace=True)\n",
       "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (24): ReLU(inplace=True)\n",
       "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (26): ReLU(inplace=True)\n",
       "  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU(inplace=True)\n",
       "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (31): ReLU(inplace=True)\n",
       "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (33): ReLU(inplace=True)\n",
       "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_cnn = vgg19(pretrained=True).features[0 : 35].to(device)\n",
    "\n",
    "for param in base_cnn.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "base_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T10:05:33.947102Z",
     "iopub.status.busy": "2025-03-15T10:05:33.946802Z",
     "iopub.status.idle": "2025-03-15T10:05:33.951257Z",
     "shell.execute_reply": "2025-03-15T10:05:33.950235Z",
     "shell.execute_reply.started": "2025-03-15T10:05:33.947051Z"
    }
   },
   "outputs": [],
   "source": [
    "OPTIMIZER = optim.Adam\n",
    "LR = 0.05\n",
    "NUM_STEPS=400\n",
    "CONTENT_WEIGHT = 1\n",
    "STYLE_WEIGHT = 10**8\n",
    "CONTENT_LAYERS = ['Conv_15', 'Conv_16']\n",
    "STYLE_LAYERS = [f'Conv_{i}' for i in range(1, 17)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = experiment(\n",
    "    base_model=base_cnn,\n",
    "    content_images=content_images[8:10], \n",
    "    style_images=style_images[4:10],\n",
    "    optimizer=OPTIMIZER, \n",
    "    lr=LR, \n",
    "    num_steps=NUM_STEPS, \n",
    "    content_weight=CONTENT_WEIGHT, \n",
    "    style_weight=STYLE_WEIGHT, \n",
    "    content_layers=CONTENT_LAYERS, \n",
    "    style_layers=STYLE_LAYERS,\n",
    "    noisy_input=False,\n",
    "    print_logs=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T10:49:42.502804Z",
     "iopub.status.busy": "2025-03-15T10:49:42.502437Z",
     "iopub.status.idle": "2025-03-15T10:49:42.508637Z",
     "shell.execute_reply": "2025-03-15T10:49:42.507707Z",
     "shell.execute_reply.started": "2025-03-15T10:49:42.502770Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avg_time': 23.758590936660767,\n",
       " 'content_score': 0.0397284357839823,\n",
       " 'style_score': 0.000604297043492969}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6835039,
     "sourceId": 10982636,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6835144,
     "sourceId": 10982763,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6837026,
     "sourceId": 10985232,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6864269,
     "sourceId": 11028608,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
