{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10982636,"sourceType":"datasetVersion","datasetId":6835039},{"sourceId":10982763,"sourceType":"datasetVersion","datasetId":6835144},{"sourceId":10985232,"sourceType":"datasetVersion","datasetId":6837026},{"sourceId":11028608,"sourceType":"datasetVersion","datasetId":6864269}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NST эксперименты","metadata":{}},{"cell_type":"markdown","source":"### Импорты","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport torchvision\nfrom torchvision.transforms import Compose, Resize, ToTensor, ToPILImage\nfrom torchvision.models import vgg19\n\nfrom PIL import Image\n\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\ntorch.set_default_device(device)\ndevice","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Классы модели-стилизатора (см. baseline)","metadata":{}},{"cell_type":"code","source":"VGG19_NORMALIZATION_MEAN = torch.tensor([0.485, 0.456, 0.406])\nVGG19_NORMALIZATION_STD = torch.tensor([0.229, 0.224, 0.225])\n\nCONTENT_LAYERS_DEFAULT = ['Conv_4']\nSTYLE_LAYERS_DEFAULT = ['Conv_1', 'Conv_2', 'Conv_3', 'Conv_4', 'Conv_5']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ContentLoss(nn.Module):\n    def __init__(self, target):\n        super(ContentLoss, self).__init__()\n        self.target = target.detach().to(device)\n        \n    def forward(self, inp):\n        self.loss = F.mse_loss(inp, self.target).to(device)\n        return inp\n    \n\nclass StyleLoss(nn.Module):\n    def __init__(self, target):\n        super(StyleLoss, self).__init__()\n        self.target = StyleLoss.gram_matrix(target).detach().to(device)\n        \n    @staticmethod\n    def gram_matrix(inp):\n        batch_size, nmaps, w, h = inp.size()\n    \n        features = inp.view(batch_size * nmaps, w * h) # Получаем для каждой карты признаков вектор размера wxh\n    \n        G = torch.mm(features, features.T)\n    \n        return G.div(batch_size * nmaps * w * h)\n\n    \n    def forward(self, inp):\n        G = StyleLoss.gram_matrix(inp).to(device)\n        self.loss = F.mse_loss(G, self.target).to(device)\n        return inp","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Normalization(nn.Module):\n    def __init__(self, mean=VGG19_NORMALIZATION_MEAN,\n                 std=VGG19_NORMALIZATION_STD):\n       super(Normalization, self).__init__()\n       self.mean = torch.tensor(mean).view(-1, 1, 1)\n       self.std = torch.tensor(std).view(-1, 1, 1)\n   \n    def forward(self, img):\n       return (img - self.mean) / self.std","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class StyleModel:\n    \"\"\"\n    Класс для создания интерфейса по переносу стиля\n    \"\"\"\n    def __init__(self, base_cnn, content_img, style_img, \n                 normalization_mean=VGG19_NORMALIZATION_MEAN, \n                 normalization_std=VGG19_NORMALIZATION_STD, \n                 content_layers=CONTENT_LAYERS_DEFAULT, \n                 style_layers=STYLE_LAYERS_DEFAULT):\n        \n        content_img = content_img.to(device)\n        style_img = style_img.to(device)\n        self.model = nn.Sequential().to(device)\n        self.content_losses = []\n        self.style_losses = []\n        \n        self.model.add_module(\"ImageNorm\", Normalization(mean=normalization_mean, std=normalization_std).to(device))\n\n        conv_counter: int = 0\n        module_name: str = \"\"\n        \n        for layer in base_cnn.children():\n            if isinstance(layer, nn.Conv2d):\n                conv_counter += 1\n                module_name = f\"Conv_{conv_counter}\"\n                \n            elif isinstance(layer, nn.ReLU):\n                module_name = f\"ReLU_{conv_counter}\"\n                layer = nn.ReLU(inplace=False)\n                \n            elif isinstance(layer, nn.MaxPool2d):\n                module_name = f\"MaxPool2d_{conv_counter}\"\n            \n            else:\n                raise ValueError\n            \n            self.model.add_module(module_name, layer.to(device))\n\n            if module_name in content_layers:\n                target = self.model(content_img).detach().to(device)\n                content_loss_module = ContentLoss(target)\n                self.content_losses.append(content_loss_module)\n                self.model.add_module(f\"ContentLoss_{conv_counter}\", content_loss_module)\n                \n            if module_name in style_layers:\n                target = self.model(style_img).detach().to(device)\n                style_loss_module = StyleLoss(target)\n                self.style_losses.append(style_loss_module)\n                self.model.add_module(f\"StyleLoss_{conv_counter}\", style_loss_module)\n                \n        self.model.to(device)\n\n    \n    def __repr__(self):\n        return f'{self.__class__.__name__}(model = {self.model})'\n\n\n    def transfer_style(self, input_image, optimizer_class=None, lr=0.05, \n                       num_steps=300, style_weight=100000, content_weight=1, print_logs:int|None=None):\n        \n        input_image = input_image.to(device)\n        \n        input_image.requires_grad = True\n        self.model.eval()\n\n        optimizer = None\n        if not optimizer_class:\n            optimizer = optim.Adam([input_image], lr=lr)\n        else:\n            optimizer = optimizer_class([input_image], lr=lr)\n\n        \n        for i in range(num_steps):\n            \n            def closure():\n                optimizer.zero_grad()\n\n                self.model(input_image)\n                content_final_loss = 0.0\n                style_final_loss = 0.0\n            \n                for content_loss in self.content_losses:\n                    content_final_loss += content_loss.loss\n\n                for style_loss in self.style_losses:\n                    style_final_loss += style_loss.loss\n\n                loss = content_weight * content_final_loss + style_weight * style_final_loss\n                loss.backward()\n\n                return loss\n                \n            optimizer.step(closure)\n            \n            with torch.no_grad():\n                input_image.clamp_(0, 1)\n\n            if print_logs:\n                if i % print_logs == 0:\n                    print(f\"Эпоха номер {i + 1}\")\n                    plt.imshow(ToPILImage()(input_image[0]))\n                    plt.show()\n                \n        with torch.no_grad():\n            input_image.clamp_(0, 1)\n            \n        return input_image","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Входные данные","metadata":{}},{"cell_type":"code","source":"import os\n\nIMSIZE = (512, 512)\n\ncontents_directory = \"/kaggle/input/images3-0/data/contents/\"\nstyles_directory = \"/kaggle/input/images3-0/data/styles/\"\n\ncontent_names = os.listdir(contents_directory) \nstyle_names = os.listdir(styles_directory)\n\ncontent_images = [Image.open(contents_directory + filename).resize(IMSIZE) for filename in content_names]\nstyle_images = [Image.open(styles_directory + filename).resize(IMSIZE) for filename in style_names]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Эксперименты","metadata":{}},{"cell_type":"markdown","source":"### 1. Базовая модель","metadata":{}},{"cell_type":"code","source":"base_cnn = vgg19(pretrained=True).features[0 : 26].to(device)\n\nfor param in base_cnn.parameters():\n    param.requires_grad = False\n\nbase_cnn","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2. Функция эксперимента","metadata":{}},{"cell_type":"code","source":"def experiment(base_model, content_images, style_images, \n               optimizer, lr, num_steps, \n               content_weight, style_weight, \n               content_layers, style_layers, \n               noisy_input: bool = False, print_logs: int|None=None):\n    \n    for content_img in content_images:\n        for style_img in style_images:\n            content = ToTensor()(content_img)[:3].unsqueeze(0)\n            style = ToTensor()(style_img)[:3].unsqueeze(0)\n        \n            input_ = torch.randn(content.data.size()) if noisy_input else content.clone()\n        \n            style_model = StyleModel(\n                base_cnn=base_model,\n                content_img=content,\n                style_img=style,\n                content_layers=content_layers,\n                style_layers=style_layers\n            )\n\n            output = style_model.transfer_style(\n                input_image=input_,\n                optimizer_class=optimizer,\n                lr=lr,\n                num_steps=num_steps,\n                content_weight=content_weight,\n                style_weight=style_weight,\n                print_logs=print_logs\n            )\n\n            fig, axes = plt.subplots(1, 3, figsize=(13, 16))\n            axes[0].set_title(\"Content\")\n            axes[1].set_title(\"Style\")\n            axes[2].set_title(\"Result\")\n            axes[0].imshow(content_img)\n            axes[1].imshow(style_img)\n            axes[2].imshow(ToPILImage()(output[0]))\n            plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Опыт 1","metadata":{}},{"cell_type":"code","source":"OPTIMIZER = optim.Adam\nLR = 0.05\nNUM_STEPS=400\nCONTENT_WEIGHT = 1\nSTYLE_WEIGHT = 10**8\nCONTENT_LAYERS = ['Conv_10', 'Conv_11', 'Conv_12']\nSTYLE_LAYERS = ['Conv_1', 'Conv_2', 'Conv_3', 'Conv_4', 'Conv_5', 'Conv_6', 'Conv_7', 'Conv_8', 'Conv_9', 'Conv_10', 'Conv_11', 'Conv_12']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"experiment(\n    base_model=base_cnn,\n    content_images=[content_images[8]], \n    style_images=style_images,\n    optimizer=OPTIMIZER, \n    lr=LR, \n    num_steps=NUM_STEPS, \n    content_weight=CONTENT_WEIGHT, \n    style_weight=STYLE_WEIGHT, \n    content_layers=CONTENT_LAYERS, \n    style_layers=STYLE_LAYERS,\n    noisy_input=False,\n    print_logs=None\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}