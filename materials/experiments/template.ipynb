{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NST эксперименты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, ToPILImage\n",
    "from torchvision.models import vgg19\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "torch.set_default_device(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Класс модели-стилизатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG19_NORMALIZATION_MEAN = torch.tensor([0.485, 0.456, 0.406])\n",
    "VGG19_NORMALIZATION_STD = torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "CONTENT_LAYERS_DEFAULT = ['Conv_4']\n",
    "STYLE_LAYERS_DEFAULT = ['Conv_1', 'Conv_2', 'Conv_3', 'Conv_4', 'Conv_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentLoss(nn.Module):\n",
    "    def __init__(self, target):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        self.target = target.detach().to(device)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        self.loss = F.mse_loss(inp, self.target).to(device)\n",
    "        return inp\n",
    "    \n",
    "\n",
    "class StyleLoss(nn.Module):\n",
    "    def __init__(self, target):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = StyleLoss.gram_matrix(target).detach().to(device)\n",
    "        \n",
    "    @staticmethod\n",
    "    def gram_matrix(inp):\n",
    "        batch_size, nmaps, w, h = inp.size()\n",
    "    \n",
    "        features = inp.view(batch_size * nmaps, w * h) # Получаем для каждой карты признаков вектор размера wxh\n",
    "    \n",
    "        G = torch.mm(features, features.T)\n",
    "    \n",
    "        return G.div(batch_size * nmaps * w * h)\n",
    "\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        G = StyleLoss.gram_matrix(inp).to(device)\n",
    "        self.loss = F.mse_loss(G, self.target).to(device)\n",
    "        return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalization(nn.Module):\n",
    "    def __init__(self, mean=VGG19_NORMALIZATION_MEAN,\n",
    "                 std=VGG19_NORMALIZATION_STD):\n",
    "       super(Normalization, self).__init__()\n",
    "       self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
    "       self.std = torch.tensor(std).view(-1, 1, 1)\n",
    "   \n",
    "    def forward(self, img):\n",
    "       return (img - self.mean) / self.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleModel:\n",
    "    \"\"\"\n",
    "    Класс для создания интерфейса по переносу стиля\n",
    "    \"\"\"\n",
    "    def __init__(self, base_cnn, content_img, style_img, \n",
    "                 normalization_mean=VGG19_NORMALIZATION_MEAN, \n",
    "                 normalization_std=VGG19_NORMALIZATION_STD, \n",
    "                 content_layers=CONTENT_LAYERS_DEFAULT, \n",
    "                 style_layers=STYLE_LAYERS_DEFAULT):\n",
    "        \n",
    "        content_img = content_img.to(device)\n",
    "        style_img = style_img.to(device)\n",
    "        self.model = nn.Sequential().to(device)\n",
    "        self.content_losses = []\n",
    "        self.style_losses = []\n",
    "        \n",
    "        self.model.add_module(\"ImageNorm\", Normalization(mean=normalization_mean, std=normalization_std).to(device))\n",
    "\n",
    "        conv_counter: int = 0\n",
    "        module_name: str = \"\"\n",
    "        \n",
    "        for layer in base_cnn.children():\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                conv_counter += 1\n",
    "                module_name = f\"Conv_{conv_counter}\"\n",
    "                \n",
    "            elif isinstance(layer, nn.ReLU):\n",
    "                module_name = f\"ReLU_{conv_counter}\"\n",
    "                layer = nn.ReLU(inplace=False)\n",
    "                \n",
    "            elif isinstance(layer, nn.MaxPool2d):\n",
    "                module_name = f\"MaxPool2d_{conv_counter}\"\n",
    "            \n",
    "            else:\n",
    "                raise ValueError\n",
    "            \n",
    "            self.model.add_module(module_name, layer.to(device))\n",
    "\n",
    "            if module_name in content_layers:\n",
    "                target = self.model(content_img).detach().to(device)\n",
    "                content_loss_module = ContentLoss(target)\n",
    "                self.content_losses.append(content_loss_module)\n",
    "                self.model.add_module(f\"ContentLoss_{conv_counter}\", content_loss_module)\n",
    "                \n",
    "            if module_name in style_layers:\n",
    "                target = self.model(style_img).detach().to(device)\n",
    "                style_loss_module = StyleLoss(target)\n",
    "                self.style_losses.append(style_loss_module)\n",
    "                self.model.add_module(f\"StyleLoss_{conv_counter}\", style_loss_module)\n",
    "                \n",
    "        self.model.to(device)\n",
    "\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__.__name__}(model = {self.model})'\n",
    "\n",
    "\n",
    "    def transfer_style(self, input_image, optimizer_class=None, lr=0.05, \n",
    "                       num_steps=300, style_weight=100000, content_weight=1, print_logs:int|None=None):\n",
    "        \n",
    "        input_image = input_image.to(device)\n",
    "        \n",
    "        input_image.requires_grad = True\n",
    "        self.model.eval()\n",
    "\n",
    "        optimizer = None\n",
    "        if not optimizer_class:\n",
    "            optimizer = optim.Adam([input_image], lr=lr)\n",
    "        else:\n",
    "            optimizer = optimizer_class([input_image], lr=lr)\n",
    "\n",
    "        \n",
    "        for i in range(num_steps):\n",
    "            \n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                self.model(input_image)\n",
    "                content_final_loss = 0.0\n",
    "                style_final_loss = 0.0\n",
    "            \n",
    "                for content_loss in self.content_losses:\n",
    "                    content_final_loss += content_loss.loss\n",
    "\n",
    "                for style_loss in self.style_losses:\n",
    "                    style_final_loss += style_loss.loss\n",
    "\n",
    "                loss = content_weight * content_final_loss + style_weight * style_final_loss\n",
    "                loss.backward()\n",
    "\n",
    "                return loss\n",
    "                \n",
    "            optimizer.step(closure)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                input_image.clamp_(0, 1)\n",
    "\n",
    "            if print_logs:\n",
    "                if i % print_logs == 0:\n",
    "                    print(f\"Эпоха номер {i + 1}\")\n",
    "                    plt.imshow(ToPILImage()(input_image[0]))\n",
    "                    plt.show()\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            input_image.clamp_(0, 1)\n",
    "            \n",
    "        return input_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Входные данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "IMSIZE = (512, 512)\n",
    "\n",
    "contents_directory = \"/kaggle/input/images3-0/data/contents/\"\n",
    "styles_directory = \"/kaggle/input/images3-0/data/styles/\"\n",
    "\n",
    "content_names = os.listdir(contents_directory) \n",
    "style_names = os.listdir(styles_directory)\n",
    "\n",
    "content_images = [Image.open(contents_directory + filename).resize(IMSIZE) for filename in content_names]\n",
    "style_images = [Image.open(styles_directory + filename).resize(IMSIZE) for filename in style_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Эксперименты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Базовая модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_cnn = vgg19(pretrained=True).features[0 : 26].to(device)\n",
    "\n",
    "for param in base_cnn.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "base_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Функция эксперимента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(base_model, content_images, style_images, \n",
    "               optimizer, lr, num_steps, \n",
    "               content_weight, style_weight, \n",
    "               content_layers, style_layers, \n",
    "               noisy_input: bool = False, print_logs: int|None=None) -> dict[str, float]:\n",
    "    \n",
    "    results: dict[str, float] = {\n",
    "        \"avg_time\": 0.0, \n",
    "        \"content_score\": 0.0,\n",
    "        \"style_score\": 0.0\n",
    "    }\n",
    "    \n",
    "    iteration_counter = 0\n",
    "    total_time_spent_on_transfering = 0.0\n",
    "\n",
    "    total_content_score = 0.0\n",
    "    total_style_score = 0.0\n",
    "    \n",
    "    for content_img in content_images:\n",
    "        for style_img in style_images:\n",
    "            \n",
    "            start = time.time()\n",
    "            \n",
    "            content = ToTensor()(content_img)[:3].unsqueeze(0)\n",
    "            style = ToTensor()(style_img)[:3].unsqueeze(0)\n",
    "        \n",
    "            input_ = torch.randn(content.data.size()) if noisy_input else content.clone()\n",
    "            \n",
    "            style_model = StyleModel(\n",
    "                base_cnn=base_model,\n",
    "                content_img=content,\n",
    "                style_img=style,\n",
    "                content_layers=content_layers,\n",
    "                style_layers=style_layers\n",
    "            )\n",
    "\n",
    "            output = style_model.transfer_style(\n",
    "                input_image=input_,\n",
    "                optimizer_class=optimizer,\n",
    "                lr=lr,\n",
    "                num_steps=num_steps,\n",
    "                content_weight=content_weight,\n",
    "                style_weight=style_weight,\n",
    "                print_logs=print_logs\n",
    "            )\n",
    "            \n",
    "            end = time.time()\n",
    "            total_time_spent_on_transfering += end - start\n",
    "            iteration_counter += 1\n",
    "\n",
    "            with torch.no_grad():\n",
    "                total_content_score += F.mse_loss(output.to(device), content.to(device)).item()\n",
    "                style_loss = StyleLoss(style)\n",
    "                style_loss(output)\n",
    "                total_style_score += style_loss.loss.item()\n",
    "\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(13, 16))\n",
    "            axes[0].set_title(\"Content\")\n",
    "            axes[1].set_title(\"Style\")\n",
    "            axes[2].set_title(\"Result\")\n",
    "            axes[0].imshow(content_img)\n",
    "            axes[1].imshow(style_img)\n",
    "            axes[2].imshow(ToPILImage()(output[0]))\n",
    "            plt.show()\n",
    "            \n",
    "    results[\"avg_time\"] = total_time_spent_on_transfering / iteration_counter\n",
    "    results[\"content_score\"] = total_content_score / iteration_counter\n",
    "    results[\"style_score\"] = total_style_score / iteration_counter\n",
    "    \n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6835039,
     "sourceId": 10982636,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6835144,
     "sourceId": 10982763,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6837026,
     "sourceId": 10985232,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6864269,
     "sourceId": 11028608,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
