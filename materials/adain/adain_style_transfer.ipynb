{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86fe2a7a-fffa-4f29-aa1d-3d51ae46f1f1",
   "metadata": {},
   "source": [
    "# Стилизация при помощи Adaptive Instance Normalization\n",
    "\n",
    "Оригинальная статья: https://openaccess.thecvf.com/content_ICCV_2017/papers/Huang_Arbitrary_Style_Transfer_ICCV_2017_paper.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0077af5-d86c-4307-9d23-024ef128cd7c",
   "metadata": {},
   "source": [
    "#### Задача: имплементация архитектуры из статьи, обучение и сравнение с алгоритмом Гатиса."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99a8d6c-2256-4dfe-b7b0-1f1d053f0b2a",
   "metadata": {},
   "source": [
    "# Содержание\n",
    "## 1. [Краткий обзор идеи](#section1)\n",
    "## 2. [Реализация архитектуры](#section2)\n",
    "- ### [Энкодер](#section2.1)\n",
    "- ### [AdaIN-блок](#section2.2)\n",
    "- ### [Декодер](#section2.3)\n",
    "- ### [Итоговый класс модели переноса стиля](#section2.4)\n",
    "## 3. [Загрузка и предобработка датасета](#section3)\n",
    "## 4. [Функции обучения модели](#section4)\n",
    "## 5. [Запуск обучения](#section5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ff0af2-4569-44b3-a4b9-550d14152ba8",
   "metadata": {},
   "source": [
    "## <a id=\"section1\"> </a> 1. Краткий обзор идеи"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff40cea8-5dcd-4028-96a9-f5ce5e0c252d",
   "metadata": {},
   "source": [
    "![](./architecture.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3472f33-cdfc-456b-b5b5-9b2733763390",
   "metadata": {},
   "source": [
    "<font size=\"4\">\n",
    "    Общая идея:\n",
    "    <ol>\n",
    "        <li> Используем первые слои VGG-19 в качестве энкодера. Получаем карты признаков для изображений контента и стиля.</li>\n",
    "        <li> Для каждого канала полученных карт признаков в отдельности считаем среднее значение и стандартное отклонение. </li>\n",
    "        <li> Нормализуем (меняем распределение) карт признаков контента согласно полученным статистикам для стиля. Получается перенос контента в распределение стиля. </li>\n",
    "        <li> Преобразованные карты признаков нужно превратить снова в изображение, поэтому они подаются на вход в декодер, задача которого сгенерировать стилизованное трёхканальное изображение с исходными размерностями. </li>\n",
    "        <li> Для обучения прогоняем полученную картинку снова через энкодер и считаем лосс по выходам энкодера. </li>\n",
    "    </ol>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94fff16-65d6-4e6b-be91-d11ec7b962c0",
   "metadata": {},
   "source": [
    "Лосс складывается из двух частей: потеря контента и потеря стиля. \n",
    "\n",
    "- `Потеря контента` - это просто MSELoss между выходом энкодера и выходом AdaIN.\n",
    "- `Потеря стиля` - это разница между статистиками стиля после прохода через энкодер и статистиками сгенерированного изображения после энкодера."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1a321f-8e40-4c30-878a-ecd76dc80774",
   "metadata": {},
   "source": [
    "### Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e3967e4-81f6-4592-b0e0-7b01a61eb7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28ebe353-2985-44e1-9f32-73096c8a180b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import vgg19\n",
    "import PIL\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5378b617-803c-4245-b66a-94ced20fc32a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134336e7-a6ba-49be-a3f5-9ecf0c1e1481",
   "metadata": {},
   "source": [
    "## <center><a id=\"section2\"></a>2. Реализация архитектуры </center>\n",
    "### <a id=\"section2.1\">2.1. Энкодер</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ddb55f-aa00-457f-9066-10c8f9f59792",
   "metadata": {},
   "source": [
    "В оригинальной статье бралась вся модель вплоть до Relu_4_1. Это 20-й слой сети VGG19. Сделаем так же."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3f3939a-339f-41a9-899c-5a53134da1ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace=True)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (17): ReLU(inplace=True)\n",
       "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg = vgg19(True).features[:21]\n",
    "\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "vgg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3749fb-1a4a-48c2-a61e-4e14531376c9",
   "metadata": {},
   "source": [
    "* блок-1: 0 - 4\n",
    "* блок-2: 5 - 9\n",
    "* блок-3: 10 - 18\n",
    "* блок-4: 19 - 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f7136d-5717-4861-ba02-e3ebe574c505",
   "metadata": {},
   "source": [
    "<font size=\"4\">Потеря стиля в оригинальной статье считается после слоёв relu_1_1, relu_2_1, relu_3_1, relu_4_1, поэтому нужно иметь возможность получения выходов с каждого из этого слоя для рассчёта лосса.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85b643b5-6769-4e91-81bb-9606a4d6b4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, base_cnn: nn.Sequential):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = base_cnn\n",
    "\n",
    "    def forward(self, inp, return_all_outputs=False):\n",
    "        outp1 = self.encoder[:2](inp) # relu_1_1\n",
    "        outp2 = self.encoder[2:7](outp1) # relu_2_1\n",
    "        outp3 = self.encoder[7:12](outp2) # relu_3_1\n",
    "        outp4 = self.encoder[12:21](outp3) # relu_4_1\n",
    "\n",
    "        # для подсчёта потери стиля\n",
    "        if return_all_outputs:\n",
    "            return outp1, outp2, outp3, outp4\n",
    "            \n",
    "        return outp4    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a35b33-eb92-494c-a66f-b1be634e7d42",
   "metadata": {},
   "source": [
    "### <a id=\"section2.2\">2.2. AdaIN-блок</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd006bf-3c5a-4998-886c-2f273ffc7e70",
   "metadata": {},
   "source": [
    "- <font size=\"4\"> блок Adaptive Instance Normalization, который получает `content_feature_maps` и `style_feature_maps`, рассчитывает для них векторы средних и стандартных отклонений (для каждого канала в отдельности), и приводит распределение `content_feature_maps` к распределению `style_feature_maps`. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908ab178-6bc8-4300-9248-257c15e6fb5a",
   "metadata": {},
   "source": [
    "![](adain.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb55f371-d069-4356-b5c2-19900200d1c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 6.3333]],\n",
       "\n",
       "         [[ 6.3333]],\n",
       "\n",
       "         [[32.1333]]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[  5.0943]],\n",
       "\n",
       "         [[  5.0943]],\n",
       "\n",
       "         [[129.7981]]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Игрушечный пример расчёта mean и std для батча изображений\n",
    "test = torch.tensor([\n",
    "    [\n",
    "        [\n",
    "            [1, 2, 3, 4, 5],\n",
    "            [5, 4, 3, 2, 1],\n",
    "            [11, 12, 13, 14, 15]\n",
    "        ],\n",
    "        [\n",
    "            [1, 2, 3, 4, 5],\n",
    "            [5, 4, 3, 2, 1],\n",
    "            [11, 12, 13, 14, 15]\n",
    "        ],\n",
    "        [\n",
    "            [1, 1, 1, 0, 0],\n",
    "            [0, 0, -35, 2, 1],\n",
    "            [0, 500, 0, 11, 0]\n",
    "        ]\n",
    "    ]\n",
    "], dtype=torch.float)\n",
    "\n",
    "batch_size = test.size()[0]\n",
    "channels = test.size()[1]\n",
    "h = test.size()[2]\n",
    "w = test.size()[3]\n",
    "\n",
    "display(test.reshape(batch_size, channels, -1).mean(dim=2).reshape(batch_size, channels, 1, 1))\n",
    "print()\n",
    "display(test.reshape(batch_size, channels, -1).std(dim=2).reshape(batch_size, channels, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc285b52-4990-4f42-917d-5290ba80e02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanStdCalculator(nn.Module):\n",
    "    \"\"\"\n",
    "    Модуль подсчёта среднего значения и стандартного отклонения по каждому каналу.\n",
    "    Пригодится в блоке AdaIN, а также при вычислении потери стиля.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(MeanStdCalculator, self).__init__()\n",
    "\n",
    "    def forward(self, feature_maps: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        batch_size = feature_maps.size()[0]\n",
    "        channels = feature_maps.size()[1]\n",
    "        \n",
    "        mean = feature_maps.reshape(batch_size, channels, -1).mean(dim=2).reshape(batch_size, channels, 1, 1)\n",
    "        std = feature_maps.reshape(batch_size, channels, -1).std(dim=2).reshape(batch_size, channels, 1, 1)\n",
    "\n",
    "        return mean, std\n",
    "        \n",
    "\n",
    "class AdaIN(nn.Module):\n",
    "    \"\"\"\n",
    "    Модуль адаптивной инстанс нормализации.\n",
    "    Преобразует карты признаков контента согласно распределения карт признаков для стиля.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(AdaIN, self).__init__()\n",
    "        self.mean_std_calc = MeanStdCalculator()\n",
    "        \n",
    "    def forward(self, content_features, style_features) -> torch.Tensor:\n",
    "        content_mean, content_std = self.mean_std_calc(content_features)\n",
    "        content_std = content_std + 1e-8\n",
    "        style_mean, style_std = self.mean_std_calc(style_features)\n",
    "        style_std = style_std + 1e-8\n",
    "\n",
    "        norm_content = (content_features - content_mean) / content_std\n",
    "        adapt_content = norm_content * style_std + style_mean\n",
    "\n",
    "        return adapt_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6aee223b-4919-4a19-b7c8-8a11fde53dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[  1.0000,   2.0000,   3.0000,   4.0000,   5.0000],\n",
       "          [  5.0000,   4.0000,   3.0000,   2.0000,   1.0000],\n",
       "          [ 11.0000,  12.0000,  13.0000,  14.0000,  15.0000]],\n",
       "\n",
       "         [[  1.0000,   2.0000,   3.0000,   4.0000,   5.0000],\n",
       "          [  5.0000,   4.0000,   3.0000,   2.0000,   1.0000],\n",
       "          [ 11.0000,  12.0000,  13.0000,  14.0000,  15.0000]],\n",
       "\n",
       "         [[  1.0000,   1.0000,   1.0000,   0.0000,   0.0000],\n",
       "          [  0.0000,   0.0000, -35.0000,   2.0000,   1.0000],\n",
       "          [  0.0000, 500.0000,   0.0000,  11.0000,   0.0000]]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapt_test = AdaIN()(test, test)\n",
    "adapt_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e92ecd-76bd-4755-ba32-44f7c28bc65a",
   "metadata": {},
   "source": [
    "### <a id=\"section2.3\">2.3. Декодер</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c832b4-c5c9-435d-8f73-e9104b8462d4",
   "metadata": {},
   "source": [
    "После блока AdaIN мы получаем выход t - нормализованные по параметрам стиля карты признаков. После этого данное представление t нужно восстановить до исходного размера изображений, получив стилизацию. \n",
    "\n",
    "В статье было предложено использовать в декодере nearest-upsampling для того, чтобы избегать появления артефактов на выходном изображении. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8e8c451-95c0-4375-8c51-e3e22f1924b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходная размерность: torch.Size([1, 3, 512, 512])\n",
      "pool1: torch.Size([1, 64, 256, 256])\n",
      "pool2: torch.Size([1, 128, 128, 128])\n",
      "pool3: torch.Size([1, 512, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# Сформируем правильную последовательность Upsampling для восстановления исходного размра изображения\n",
    "\n",
    "# 1. Возьмем тестовую картинку и приведем ее к размеру 512х512\n",
    "test_img = Image.open(\"../online-nst/data/contents/dancing.jpg\").resize((512, 512))\n",
    "test_img = transforms.ToTensor()(test_img).unsqueeze(0)\n",
    "\n",
    "# 2. Прогоним её через слои VGG-19, чтобы увидеть, как меняются ее размерности \n",
    "# с каждым max-pool размерность должна уменьшаться вдвое, а свертка с kernel_size=3, padding=1, stride=1 не должна менять размер. Проверим:\n",
    "print(f\"Исходная размерность: {test_img.size()}\")\n",
    "\n",
    "pool1 = vgg[:5](test_img)\n",
    "print(f\"pool1: {pool1.size()}\")\n",
    "\n",
    "pool2 = vgg[5:10](pool1)\n",
    "print(f\"pool2: {pool2.size()}\")\n",
    "\n",
    "pool3 = vgg[10:](pool2)\n",
    "print(f\"pool3: {pool3.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a6c1e4c-282e-4621-a04c-d256c7e78e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Выходная размерность torch.Size([1, 3, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "# 3. Добавим последовательность upsample блоков для восстановления размерности. Для регулирования количества каналов используем Conv2d\n",
    "upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "conv1 = nn.Conv2d(512, 128, kernel_size=3, stride=1, padding=1)\n",
    "conv2 = nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1)\n",
    "conv3 = nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "result = conv3(upsample(conv2(upsample(conv1(upsample(pool3))))))\n",
    "print(f\"Выходная размерность {result.size()}\")\n",
    "\n",
    "assert result.size() == test_img.size(), \"Размерности должны были совпасть\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed138008-abdb-469d-bf8a-c0926dbc32ba",
   "metadata": {},
   "source": [
    "Теперь можно реализовывать декодер..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcf0588b-8d2b-4444-bc51-ed98cba62eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Декодер должен хорошо научиться восстанавливать изображения,\n",
    "    для этого понадобится использовать бОльшее количество свёрток чем 3.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        )\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        )\n",
    "\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, inp):\n",
    "        return self.block3(self.block2(self.block1(inp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27bfbe69-a65c-419a-bafe-df70a7cddc86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 512, 512])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = vgg(test_img)\n",
    "decoded = Decoder()(encoded)\n",
    "\n",
    "decoded.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17dcec0-977f-48d2-9bbd-68f21b8a7cbb",
   "metadata": {},
   "source": [
    "### <a id=\"section2.4\">2.4. Итоговый класс модели переноса стиля</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92c9ac6a-1e63-455d-96ae-be8e710e47df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdainStyleTransferModel(nn.Module):\n",
    "    def __init__(self, base_cnn: nn.Sequential):\n",
    "        super(AdainStyleTransferModel, self).__init__()\n",
    "        self.encoder = Encoder(base_cnn=base_cnn)\n",
    "        self.adain = AdaIN()\n",
    "        self.decoder = Decoder()\n",
    "        self.mean_std_calc = MeanStdCalculator()\n",
    "\n",
    "    \n",
    "    def stylize(self, content: torch.Tensor, style: torch.Tensor, alpha: float = 1.0):\n",
    "        \"\"\"\n",
    "        Функция стилизации, которая будет применяться при инференсе.\n",
    "        * alpha - число [0, 1], степень переноса стиля\n",
    "        \"\"\"\n",
    "        content_features = self.encoder(content, return_all_outputs=False) # карты признаков контента с последнего слоя энкодера \n",
    "        style_features = self.encoder(style, return_all_outputs=False) # карты признаков стиля с последнего слоя энкодера\n",
    "        \n",
    "        adain_features = self.adain(content_features, style_features) # модифицированные карты признаков контента (стилизованные)\n",
    "        adain_features = alpha * adain_features + (1 - alpha) * content_features # контроль степени стилизации при помощи alpha\n",
    "        \n",
    "        outp = self.decoder(adain_features) # получение стилизованного изображения исходного (512х512) размера\n",
    "        \n",
    "        return outp\n",
    "\n",
    "\n",
    "    def __content_loss(self, result_enc_outp, adain_features):\n",
    "        \"\"\"\n",
    "        * result_enc_outp - выход энкодера для D(adain_content), где D - декодер\n",
    "        * adain_content - выход блока AdaIN, преобразованные фичи изначального контент-изображения\n",
    "        \"\"\"\n",
    "        return torch.nn.MSELoss(reduction='mean')(result_enc_outp, adain_features)\n",
    "\n",
    "    \n",
    "    def __style_loss(self, result_enc_outputs, style_enc_outputs):\n",
    "        \"\"\"\n",
    "        * result_enc_outputs - выходы всех рассмотренных выше слоев энкодера для полученной декодером стилизации\n",
    "        * style_enc_outputs - выходы всех рассмотренных выше слоев энкодера для изображения стиля\n",
    "        \"\"\"\n",
    "        style_loss_ = 0.0\n",
    "        for result_outp, style_outp in zip(result_enc_outputs, style_enc_outputs):\n",
    "            result_outp_mean, result_outp_std = self.mean_std_calc(result_outp)\n",
    "            style_outp_mean, style_outp_std = self.mean_std_calc(style_outp)\n",
    "\n",
    "            style_loss_ += torch.nn.MSELoss(reduction='mean')(result_outp_mean, style_outp_mean) + torch.nn.MSELoss(reduction='mean')(result_outp_std, style_outp_std)\n",
    "            \n",
    "        return style_loss_\n",
    "        \n",
    "\n",
    "    def forward(self, content_images, style_images, alpha=1.0, style_weight=10):\n",
    "        content_features = self.encoder(content_images)\n",
    "        style_features = self.encoder(style_images)\n",
    "        \n",
    "        adain_features = self.adain(content_features, style_features)\n",
    "        adain_features = alpha * adain_features + (1 - alpha) * content_features\n",
    "        \n",
    "        result = self.decoder(adain_features)                 \n",
    "\n",
    "        result_features = self.encoder(result, return_all_outputs=False)    \n",
    "        result_all_features = self.encoder(result, return_all_outputs=True)   \n",
    "        style_all_features = self.encoder(style_images, return_all_outputs=True)   \n",
    "\n",
    "        \n",
    "        content_loss = self.__content_loss(result_features, adain_features)     \n",
    "        style_loss = self.__style_loss(result_all_features, style_all_features)  \n",
    "\n",
    "        loss = content_loss + style_weight * style_loss\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ce77f67-1af7-4d1e-81b1-82fab31fb51a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "AdainStyleTransferModel                  --\n",
       "├─Encoder: 1-1                           --\n",
       "│    └─Sequential: 2-1                   --\n",
       "│    │    └─Conv2d: 3-1                  (1,792)\n",
       "│    │    └─ReLU: 3-2                    --\n",
       "│    │    └─Conv2d: 3-3                  (36,928)\n",
       "│    │    └─ReLU: 3-4                    --\n",
       "│    │    └─MaxPool2d: 3-5               --\n",
       "│    │    └─Conv2d: 3-6                  (73,856)\n",
       "│    │    └─ReLU: 3-7                    --\n",
       "│    │    └─Conv2d: 3-8                  (147,584)\n",
       "│    │    └─ReLU: 3-9                    --\n",
       "│    │    └─MaxPool2d: 3-10              --\n",
       "│    │    └─Conv2d: 3-11                 (295,168)\n",
       "│    │    └─ReLU: 3-12                   --\n",
       "│    │    └─Conv2d: 3-13                 (590,080)\n",
       "│    │    └─ReLU: 3-14                   --\n",
       "│    │    └─Conv2d: 3-15                 (590,080)\n",
       "│    │    └─ReLU: 3-16                   --\n",
       "│    │    └─Conv2d: 3-17                 (590,080)\n",
       "│    │    └─ReLU: 3-18                   --\n",
       "│    │    └─MaxPool2d: 3-19              --\n",
       "│    │    └─Conv2d: 3-20                 (1,180,160)\n",
       "│    │    └─ReLU: 3-21                   --\n",
       "├─AdaIN: 1-2                             --\n",
       "│    └─MeanStdCalculator: 2-2            --\n",
       "├─Decoder: 1-3                           --\n",
       "│    └─Sequential: 2-3                   --\n",
       "│    │    └─Conv2d: 3-22                 1,179,904\n",
       "│    │    └─ReLU: 3-23                   --\n",
       "│    │    └─Conv2d: 3-24                 590,080\n",
       "│    │    └─ReLU: 3-25                   --\n",
       "│    │    └─Upsample: 3-26               --\n",
       "│    └─Sequential: 2-4                   --\n",
       "│    │    └─Conv2d: 3-27                 590,080\n",
       "│    │    └─ReLU: 3-28                   --\n",
       "│    │    └─Conv2d: 3-29                 590,080\n",
       "│    │    └─ReLU: 3-30                   --\n",
       "│    │    └─Conv2d: 3-31                 295,040\n",
       "│    │    └─ReLU: 3-32                   --\n",
       "│    │    └─Upsample: 3-33               --\n",
       "│    └─Sequential: 2-5                   --\n",
       "│    │    └─Conv2d: 3-34                 147,584\n",
       "│    │    └─ReLU: 3-35                   --\n",
       "│    │    └─Conv2d: 3-36                 73,792\n",
       "│    │    └─ReLU: 3-37                   --\n",
       "│    │    └─Conv2d: 3-38                 36,928\n",
       "│    │    └─ReLU: 3-39                   --\n",
       "│    │    └─Upsample: 3-40               --\n",
       "│    │    └─Conv2d: 3-41                 1,731\n",
       "├─MeanStdCalculator: 1-4                 --\n",
       "=================================================================\n",
       "Total params: 7,010,947\n",
       "Trainable params: 3,505,219\n",
       "Non-trainable params: 3,505,728\n",
       "================================================================="
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "model = AdainStyleTransferModel(vgg)\n",
    "\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e037459e-d350-450d-a567-7349612bb779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.1078, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проверим работоспособность forward-pass\n",
    "loss = model(test_img, test_img)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4f83d38-5e0c-486c-b98a-5e92d21d6f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6043272018432617\n"
     ]
    }
   ],
   "source": [
    "# проверим работоспособность метода stylize и заодно замерим время инференса\n",
    "import time\n",
    "\n",
    "a = time.time()\n",
    "\n",
    "model.stylize(test_img, test_img)\n",
    "\n",
    "b = time.time()\n",
    "\n",
    "print(b - a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89f997e-52b4-4108-a080-66436fa8268e",
   "metadata": {},
   "source": [
    "## <center><a id=\"section3\"></a>3. Загрузка и предобработка датасета </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124a8c14-8f08-48a6-a046-848cc61fdb80",
   "metadata": {},
   "source": [
    "<font size=\"4\">\n",
    "Для обучения модели были взяты следующие датасеты:\n",
    "<ul>\n",
    "    <li>MsCoco - изображения контента (https://www.kaggle.com/datasets/hariwh0/ms-coco-dataset) </li>\n",
    "    <li>Wiki-art - изображения стилей (https://www.kaggle.com/datasets/steubk/wikiart)</li>\n",
    "</ul>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f355849-647f-4cb0-8d51-d4f03c3845d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfbae7e2-884d-49a5-8424-e2bc583ccdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "IMSIZE = (512, 512)\n",
    "CROP_SIZE = (256, 256)\n",
    "\n",
    "VGG19_NORMALIZATION_MEAN = torch.tensor([0.485, 0.456, 0.406])\n",
    "VGG19_NORMALIZATION_STD = torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "# Тренировочные данные\n",
    "train_contents_directory = \"../online-nst/data/contents/\"\n",
    "train_styles_directory = \"../online-nst/data/styles/\"\n",
    "\n",
    "train_content_names = os.listdir(train_contents_directory) \n",
    "train_style_names = os.listdir(train_styles_directory)\n",
    "\n",
    "train_content_paths = [train_contents_directory + i for i in train_content_names]\n",
    "train_style_paths = [train_styles_directory + i for i in train_style_names]\n",
    "\n",
    "\n",
    "# Тестовые данные\n",
    "test_contents_directory = \"../online-nst/data/contents/\"\n",
    "test_styles_directory = \"../online-nst/data/styles/\"\n",
    "\n",
    "test_content_names = os.listdir(test_contents_directory) \n",
    "test_style_names = os.listdir(test_styles_directory)\n",
    "\n",
    "test_content_paths = [test_contents_directory + i for i in test_content_names]\n",
    "test_style_paths = [test_styles_directory + i for i in test_style_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4e72b76-ac33-4700-828b-5b5a3a2d03fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagesTrainDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Загрузка и предобработка изображений для тренировки.\n",
    "    Нужно провести следующие преобразования над изображениями:\n",
    "    * resize -> (512, 512)\n",
    "    * random_crop -> (256, 256)\n",
    "    * преобразование к тензору\n",
    "    * нормализация согласно значениям для VGG-19\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, files: list[str]):\n",
    "        super(ImagesTrainDataset, self).__init__()\n",
    "        \n",
    "        self.files = sorted(files)\n",
    "        self.len_ = len(self.files)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len_\n",
    "\n",
    "\n",
    "    def load_sample(self, file):\n",
    "        with Image.open(file) as image:\n",
    "            image.load()\n",
    "            return image.resize(IMSIZE)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.load_sample(self.files[index])\n",
    "        x = transforms.RandomCrop(CROP_SIZE)(x)\n",
    "        x = transforms.ToTensor()(x)[:3] # учитываем, что некоторые картинки могут иметь 4 канала, из-за чего нормализация может сломаться\n",
    "        x = transforms.Normalize(VGG19_NORMALIZATION_MEAN, VGG19_NORMALIZATION_STD)(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2396da79-f108-471f-b603-fe5d146fa7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagesTestDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Загрузка и предобработка изображений для тестирования.\n",
    "    Нужно провести следующие преобразования над изображениями:\n",
    "    * resize -> (512, 512)\n",
    "    * преобразование к тензору\n",
    "    * нормализация согласно значениям для VGG-19\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, files: list[str]):\n",
    "        super(ImagesTestDataset, self).__init__()\n",
    "        \n",
    "        self.files = sorted(files)\n",
    "        self.len_ = len(self.files)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len_\n",
    "\n",
    "\n",
    "    def load_sample(self, file):\n",
    "        with Image.open(file) as image:\n",
    "            image.load()\n",
    "            return image.resize(IMSIZE)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.load_sample(self.files[index])\n",
    "        x = transforms.ToTensor()(x)[:3] # учитываем, что некоторые картинки могут иметь 4 канала, из-за чего нормализация может сломаться\n",
    "        x = transforms.Normalize(VGG19_NORMALIZATION_MEAN, VGG19_NORMALIZATION_STD)(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecf54a6b-01eb-4a7e-a13f-cb1b9b01e180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denorm_images(images):\n",
    "    \"\"\"\n",
    "    Денормализация изображений на выходе.\n",
    "    Args:\n",
    "        * images (torch.Tensor) : батч нормализованных картинок\n",
    "    \"\"\"\n",
    "    means = torch.tensor(VGG19_NORMALIZATION_MEAN).reshape(1, 3, 1, 1) \n",
    "    stds = torch.tensor(VGG19_NORMALIZATION_STD).reshape(1, 3, 1, 1) \n",
    "\n",
    "    return images * stds + means  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44f75ecc-e5ce-44fb-8b5c-a8cb13c9cc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_content_dataset = ImagesTrainDataset(train_content_paths)\n",
    "train_style_dataset = ImagesTrainDataset(train_style_paths)\n",
    "\n",
    "test_content_dataset = ImagesTestDataset(test_content_paths)\n",
    "test_style_dataset = ImagesTestDataset(test_style_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cfdaaf3-de32-4a23-865d-0b934b5e13d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_content_dataloader = DataLoader(dataset=train_content_dataset, batch_size=1)\n",
    "train_style_dataloader = DataLoader(dataset=train_style_dataset, batch_size=1)\n",
    "\n",
    "test_content_dataloader = DataLoader(dataset=test_content_dataset, batch_size=1)\n",
    "test_style_dataloader = DataLoader(dataset=test_style_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f93ab16-506a-4ee4-8d94-650b8fbe6bf2",
   "metadata": {},
   "source": [
    "____________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b023a31-d6eb-4110-a20f-2945a4074132",
   "metadata": {},
   "source": [
    "## <center><a id=\"section4\"></a>4. Функции обучения модели. </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20d84efb-ffc4-49d9-80a8-a8a5a2193766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, scheduler, device, \n",
    "                train_content_loader, train_style_loader, \n",
    "                checkpoints_path: str = \"./checkpoints\", save_freq: int = 1000, epoch_number: int = 1):\n",
    "    model.train()\n",
    "    \n",
    "    style_iter = iter(train_style_loader)\n",
    "    \n",
    "    epoch_losses = []\n",
    "    \n",
    "    for i, content in tqdm(enumerate(train_content_loader)):\n",
    "        try:\n",
    "            content = content.to(device)\n",
    "            style = next(style_iter)\n",
    "            style = style.to(device)\n",
    "            \n",
    "            loss = model(content, style) # в батче у нас по одной картинке\n",
    "            \n",
    "            epoch_losses.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                scheduler.step()\n",
    "\n",
    "            if i % save_freq == 0:\n",
    "                torch.save({\n",
    "                    'epoch': epoch_number,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss\n",
    "                }, f\"{checkpoints_path}/epoch_{epoch_number}_iter_{i}.pt\")\n",
    "        \n",
    "        except (OSError, PIL.Image.DecompressionBombError) as e:\n",
    "            print(f\"Skipping batch due to truncated image file.\")\n",
    "            continue\n",
    "            \n",
    "        except StopIteration as e:\n",
    "            style_iter = iter(style_dl)\n",
    "            continue\n",
    "    epoch_losses = torch.tensor(epoch_losses, dtype=torch.float)\n",
    "    \n",
    "    return epoch_losses.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb66f991-2cc4-48ab-86e6-65e3b6262a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(model, device, test_content_loader, test_style_loader, test_epoch_dir):\n",
    "    model.eval()\n",
    "    \n",
    "    style_iter = iter(test_style_loader)\n",
    "    \n",
    "    for i, content in enumerate(test_content_loader):\n",
    "        try:\n",
    "            content = content.to(device)\n",
    "            style = next(style_iter)\n",
    "            style = style.to(device)\n",
    "            \n",
    "            output = model.stylize(content, style)\n",
    "            output = denorm_images(output)\n",
    "            output = transforms.ToPILImage()(output[0])\n",
    "            output.save(f\"{test_epoch_dir}/{i}.jpg\")\n",
    "            \n",
    "        \n",
    "        except (OSError, PIL.Image.DecompressionBombError) as e:\n",
    "            print(f\"Skipping batch due to truncated image file.\")\n",
    "            continue\n",
    "            \n",
    "        except StopIteration as e:\n",
    "            style_iter = iter(style_dl)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5dee2fb0-5ff6-4c4b-a769-55125f19dcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train(model, optimizer, scheduler, device, \n",
    "          train_content_loader: torch.utils.data.DataLoader, train_style_loader: torch.utils.data.DataLoader,\n",
    "          test_content_loader: torch.utils.data.DataLoader, test_style_loader: torch.utils.data.DataLoader,\n",
    "          num_steps: int, epoch_number: int = 1, checkpoints_path: str = \"./checkpoints\", save_freq: int = 1000, \n",
    "          test_outp_path: str = './test-outp'):\n",
    "    \"\"\"\n",
    "    Параметры:\n",
    "    * model - модель переноса стиля с замороженными слоями энкодера\n",
    "    * optimizer - оптимизатор декодера модели \n",
    "    * scheduler - выполняет уменьшение learning_rate каждые 100 итерации, если не None\n",
    "    * train_content_loader, train_style_loader - дата лоадеры\n",
    "    * num_steps - количество эпох\n",
    "    * epoch_number - номер текущей эпохи\n",
    "    * checkpoint_path - путь до папки с чекпойтами модели\n",
    "    * save_freq - частота сохранения модели, default=1000 - сохранять состояние модели каждые 1000 итераций\n",
    "    \"\"\"\n",
    "    losses_list = []\n",
    "    \n",
    "    for step in tqdm(range(num_steps)):\n",
    "        # train one epoch\n",
    "        epoch_loss = train_epoch(model, optimizer, scheduler, device, \n",
    "                                 train_content_loader, train_style_loader, \n",
    "                                 checkpoints_path, save_freq, epoch_number + step)\n",
    "        losses_list.append(epoch_loss)\n",
    "        \n",
    "        print(f\"Epoch-{epoch_number + step}. Loss = {epoch_loss}\")\n",
    "\n",
    "        if test_outp_path:\n",
    "            test_epoch_dir = os.path.join(test_outp_path, str(epoch_number + step))\n",
    "            \n",
    "            if not os.path.exists(test_epoch_dir):\n",
    "                os.makedirs(test_epoch_dir)\n",
    "                \n",
    "            test_epoch(model, device, test_content_dataloader, test_style_dataloader, test_epoch_dir)\n",
    "        \n",
    "    return losses_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f4fe66-da5a-4df8-8972-d3df23ba0f6e",
   "metadata": {},
   "source": [
    "______________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e703946-1e62-4ba3-9615-dbd3eec9717b",
   "metadata": {},
   "source": [
    "## <center><a id=\"section5\"></a>5. Обучение модели.</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b095ba9b-c09b-438a-b066-2b86376f58de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab352f7b-049b-47ad-a685-880ce5544b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "AdainStyleTransferModel                  --\n",
       "├─Encoder: 1-1                           --\n",
       "│    └─Sequential: 2-1                   --\n",
       "│    │    └─Conv2d: 3-1                  (1,792)\n",
       "│    │    └─ReLU: 3-2                    --\n",
       "│    │    └─Conv2d: 3-3                  (36,928)\n",
       "│    │    └─ReLU: 3-4                    --\n",
       "│    │    └─MaxPool2d: 3-5               --\n",
       "│    │    └─Conv2d: 3-6                  (73,856)\n",
       "│    │    └─ReLU: 3-7                    --\n",
       "│    │    └─Conv2d: 3-8                  (147,584)\n",
       "│    │    └─ReLU: 3-9                    --\n",
       "│    │    └─MaxPool2d: 3-10              --\n",
       "│    │    └─Conv2d: 3-11                 (295,168)\n",
       "│    │    └─ReLU: 3-12                   --\n",
       "│    │    └─Conv2d: 3-13                 (590,080)\n",
       "│    │    └─ReLU: 3-14                   --\n",
       "│    │    └─Conv2d: 3-15                 (590,080)\n",
       "│    │    └─ReLU: 3-16                   --\n",
       "│    │    └─Conv2d: 3-17                 (590,080)\n",
       "│    │    └─ReLU: 3-18                   --\n",
       "│    │    └─MaxPool2d: 3-19              --\n",
       "│    │    └─Conv2d: 3-20                 (1,180,160)\n",
       "│    │    └─ReLU: 3-21                   --\n",
       "├─AdaIN: 1-2                             --\n",
       "│    └─MeanStdCalculator: 2-2            --\n",
       "├─Decoder: 1-3                           --\n",
       "│    └─Sequential: 2-3                   --\n",
       "│    │    └─Conv2d: 3-22                 1,179,904\n",
       "│    │    └─ReLU: 3-23                   --\n",
       "│    │    └─Conv2d: 3-24                 590,080\n",
       "│    │    └─ReLU: 3-25                   --\n",
       "│    │    └─Upsample: 3-26               --\n",
       "│    └─Sequential: 2-4                   --\n",
       "│    │    └─Conv2d: 3-27                 590,080\n",
       "│    │    └─ReLU: 3-28                   --\n",
       "│    │    └─Conv2d: 3-29                 590,080\n",
       "│    │    └─ReLU: 3-30                   --\n",
       "│    │    └─Conv2d: 3-31                 295,040\n",
       "│    │    └─ReLU: 3-32                   --\n",
       "│    │    └─Upsample: 3-33               --\n",
       "│    └─Sequential: 2-5                   --\n",
       "│    │    └─Conv2d: 3-34                 147,584\n",
       "│    │    └─ReLU: 3-35                   --\n",
       "│    │    └─Conv2d: 3-36                 73,792\n",
       "│    │    └─ReLU: 3-37                   --\n",
       "│    │    └─Conv2d: 3-38                 36,928\n",
       "│    │    └─ReLU: 3-39                   --\n",
       "│    │    └─Upsample: 3-40               --\n",
       "│    │    └─Conv2d: 3-41                 1,731\n",
       "├─MeanStdCalculator: 1-4                 --\n",
       "=================================================================\n",
       "Total params: 7,010,947\n",
       "Trainable params: 3,505,219\n",
       "Non-trainable params: 3,505,728\n",
       "================================================================="
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_cnn = vgg19(True).features[:21]\n",
    "\n",
    "for param in base_cnn.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "LR = 0.001\n",
    "GAMMA = 0.9995\n",
    "    \n",
    "model = AdainStyleTransferModel(base_cnn).to(device)\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=LR)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=GAMMA)\n",
    "\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a61443f-ffa1-47c3-b21c-e6bca70e2e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: невозможно создать каталог «./checkpoints»: Файл существует\n",
      "mkdir: невозможно создать каталог «./test-outp»: Файл существует\n"
     ]
    }
   ],
   "source": [
    "!mkdir ./checkpoints\n",
    "!mkdir ./test-outp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb48ea6d-0373-4831-b9ad-0f35e89734fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdd162fd0e834d95b8c432b57c9342e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "316cbfe979c44f2fba1ea1732d9786d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-1. Loss = 305.49224853515625\n"
     ]
    }
   ],
   "source": [
    "losses = train(\n",
    "    model, optimizer, scheduler, device,\n",
    "    train_content_loader=train_content_dataloader,\n",
    "    train_style_loader=train_style_dataloader,\n",
    "    test_content_loader=test_content_dataloader,\n",
    "    test_style_loader=test_style_dataloader,\n",
    "    num_steps=1,\n",
    "    epoch_number=1,\n",
    "    checkpoints_path='./checkpoints',\n",
    "    save_freq=10\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
